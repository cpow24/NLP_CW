{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tsv_to_conll.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!pip install pyconll\n",
        "!git clone https://github.com/cpow24/long-doc-coref.git\n",
        "!pip install transformers==4.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1LMxivmMUiy",
        "outputId": "b2316e3b-bbb5-423f-de07-327836e0363d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "fatal: destination path 'long-doc-coref' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers==4.2.2 in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (0.0.49)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CbbmAz-KyU1",
        "outputId": "432e26d3-8019-42ca-bd60-21e0684c8b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('long-doc-coref/src')\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import conllu\n",
        "from io import open\n",
        "from conllu import parse_tree_incr\n",
        "import pyconll\n",
        "from inference.inference import Inference\n",
        "from  inference.tokenize_doc import *\n",
        "from transformers import BertTokenizerFast\n",
        "from  inference.tokenize_doc import *\n",
        "import torch\n",
        "import math\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "1uIABdLTkMkB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inverse mapping of tokenized text to list text\n",
        "\"\"\"\n",
        "def inv_map(tokenized_text):\n",
        "  text = []\n",
        "  for i in tokenized_text:\n",
        "    if len(i) < 3:\n",
        "      text.append(i)\n",
        "    elif i[0:2] != '##':\n",
        "      text.append(i)\n",
        "    else:\n",
        "      text[-1] = text[-1] + i[2:]\n",
        "  return text"
      ],
      "metadata": {
        "id": "gYfss5V4hckd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns cluster_id for given mention\n",
        "def get_mention_idx(cluster_id, mentioned):\n",
        "  if mentioned == True:\n",
        "    val = cluster_id\n",
        "  else:\n",
        "    val = 'non-mention' \n",
        "  return val"
      ],
      "metadata": {
        "id": "MUz21zQ_Pj1Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recursively extract mention indices and their associated tokens\n",
        "def extract_mentions(idx, mention_vals, tokens, mention_idx_list, mention_tuple, end_patt):\n",
        "  mention_idx_list.append(idx)\n",
        "  mention_tuple += tokens[idx]\n",
        "\n",
        "  #List potentially containing mention end pattern\n",
        "  search_vals = re.findall(r'\\d+\\)', mention_vals[idx])\n",
        "\n",
        "  #End recursion if end pattern found\n",
        "  if (end_patt in search_vals):\n",
        "    return mention_idx_list, mention_tuple\n",
        "  \n",
        "  else:\n",
        "    return extract_mentions(idx + 1, mention_vals, tokens, mention_idx_list, mention_tuple, end_patt)"
      ],
      "metadata": {
        "id": "NIke95cTP6YO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = ['1023_bleak_house_brat', '105_persuasion_brat', '1064_the_masque_of_the_red_death_brat', '110_tess_of_the_durbervilles_a_pure_woman_brat',\n",
        "'11231_bartleby_the_scrivener_a_story_of_wallstreet_brat', '113_the_secret_garden_brat', '1155_the_secret_adversary_brat', '11_alices_adventures_in_wonderland_brat',\n",
        "'1206_the_flying_u_ranch_brat', '120_treasure_island_brat', '1245_night_and_day_brat', '1260_jane_eyre_an_autobiography_brat', '12677_personality_plus_some_experiences_of_emma_mcchesney_and_her_son_jock_brat',\n",
        "'1327_elizabeth_and_her_german_garden_brat', '1342_pride_and_prejudice_brat', '1400_great_expectations_brat', '145_middlemarch_brat', '15265_the_quest_of_the_silver_fleece_a_novel_brat',\n",
        "'155_the_moonstone_brat', '158_emma_brat', '160_the_awakening_and_selected_short_stories_brat', '16357_mary_a_fiction_brat', '1661_the_adventures_of_sherlock_holmes_brat',\n",
        "'1695_the_man_who_was_thursday_a_nightmare_brat', '171_charlotte_temple_brat', '174_the_picture_of_dorian_gray_brat', '18581_adrift_in_new_york_tom_and_florence_braving_the_world_brat', \n",
        "'2005_piccadilly_jim_brat', '2084_the_way_of_all_flesh_brat', '208_daisy_miller_a_study_brat', '2095_clotelle_a_tale_of_the_southern_states_brat', '209_the_turn_of_the_screw_brat',\n",
        "'215_the_call_of_the_wild_brat', '2166_king_solomons_mines_brat', '217_sons_and_lovers_brat', '219_heart_of_darkness_brat', '233_sister_carrie_a_novel_brat', '238_dear_enemy_brat',\n",
        "'2489_moby_dick_brat', '24_o_pioneers_brat', '2641_a_room_with_a_view_brat', '271_black_beauty_brat', '2775_the_good_soldier_brat', '27_far_from_the_madding_crowd_brat',\n",
        "'2807_to_have_and_to_hold_brat', '2814_dubliners_brat', '2852_the_hound_of_the_baskervilles_brat', '2891_howards_end_brat', '3268_the_mysteries_of_udolpho_brat', '32_herland_brat',\n",
        "'33_the_scarlet_letter_brat', '3457_the_man_of_the_forest_brat', '345_dracula_brat', '351_of_human_bondage_brat', '367_country_of_the_pointed_firs_brat', '36_the_war_of_the_worlds_brat',\n",
        "'4051_lady_bridget_in_the_nevernever_land_a_story_of_australian_life_brat', '41286_miss_marjoribanks_brat', '41_the_legend_of_sleepy_hollow_brat', '4217_a_portrait_of_the_artist_as_a_young_man_brat',\n",
        "'4276_north_and_south_brat', '4300_ulysses_brat', '432_the_ambassadors_brat', '434_the_circular_staircase_brat', '44_the_song_of_the_lark_brat', '45_anne_of_green_gables_brat',\n",
        "'472_the_house_behind_the_cedars_brat', '502_desert_gold_brat', '514_little_women_brat', '521_the_life_and_adventures_of_robinson_crusoe_brat', '5230_the_invisible_man_a_grotesque_romance_brat',\n",
        "'5348_ragged_dick_or_street_life_in_new_york_with_the_bootblacks_brat', '541_the_age_of_innocence_brat', '543_main_street_brat', '550_silas_marner_brat', '599_vanity_fair_brat',\n",
        "'6053_evelina_or_the_history_of_a_young_ladys_entrance_into_the_world_brat', '60_the_scarlet_pimpernel_brat', '62_a_princess_of_mars_brat', '6593_history_of_tom_jones_a_foundling_brat', \n",
        "'711_allan_quatermain_brat', '730_oliver_twist_brat', '73_the_red_badge_of_courage_an_episode_of_the_american_civil_war_brat', '74_the_adventures_of_tom_sawyer_brat',\n",
        "'766_david_copperfield_brat', '768_wuthering_heights_brat', '76_adventures_of_huckleberry_finn_brat', '77_the_house_of_the_seven_gables_brat', '78_tarzan_of_the_apes_brat',\n",
        "'805_this_side_of_paradise_brat', '829_gullivers_travels_into_several_remote_nations_of_the_world_brat', '84_frankenstein_or_the_modern_prometheus_brat', '876_life_in_the_ironmills_or_the_korl_woman_brat',\n",
        "'8867_the_magnificent_ambersons_brat', '932_the_fall_of_the_house_of_usher_brat', '940_the_last_of_the_mohicans_a_narrative_of_1757_brat', '95_the_prisoner_of_zenda_brat',\n",
        "'969_the_tenant_of_wildfell_hall_brat', '974_the_secret_agent_a_simple_tale_brat', '9830_the_beautiful_and_damned_brat']"
      ],
      "metadata": {
        "id": "txDXPHVCGnFt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in file_list:\n",
        "  \n",
        "  #Reading files\n",
        "  conll_file_str = f'gdrive/My Drive/Colab_Notebooks/coref_extraction/conll/{file}.conll'\n",
        "  tsv_file_str = f'gdrive/My Drive/Colab_Notebooks/coref_extraction/tsv/{file}.ann'\n",
        "\n",
        "  conll_df = pd.read_csv(conll_file_str, sep='\\t', header=None, quoting=csv.QUOTE_NONE, skiprows=1)\n",
        "  tsv_df = pd.read_csv(tsv_file_str, sep='\\t', header=None)\n",
        "\n",
        "  #Naming dataframe columns\n",
        "  conll_df.columns = ['file', 'blank_id', 'sent_id', 'word', 'f1', 'f2','f3', 'f4','f5', 'f6', 'f7', 'f8', 'cluster_id']\n",
        "  tsv_df.columns = ['cat', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6','mention', 'ment_type','ment_type_2']\n",
        "\n",
        "  #Retaining only mentions\n",
        "  tsv_df = tsv_df[tsv_df.iloc[:,0] == 'MENTION']\n",
        "\n",
        "  #Removing last row as it doesnt contain data\n",
        "  conll_df.drop(conll_df.tail(1).index,inplace=True)\n",
        "\n",
        "  #Converting to lower case for string matching\n",
        "  conll_df['lower'] = conll_df.iloc[:,3].apply(lambda x: x.lower())\n",
        "  tsv_df['lower'] = tsv_df.iloc[:,6].apply(lambda x: x.lower())\n",
        "\n",
        "  #Converting text to a tuple of tokens\n",
        "  tsv_df['tokens'] = tsv_df['lower'].apply(lambda x: tuple(inv_map(flatten(get_tokenized_doc(x, tokenizer)['sentences']))))\n",
        "  conll_df['tokens'] = conll_df['lower'].apply(lambda x: tuple(inv_map(flatten(get_tokenized_doc(x, tokenizer)['sentences']))))\n",
        "\n",
        "  #Resetting indices\n",
        "  tsv_df.reset_index(drop=True, inplace=True)\n",
        "  conll_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  #Boolean indicator for mentions\n",
        "  conll_df['mention'] = conll_df.iloc[:,12].notnull()\n",
        "\n",
        "  #Mention id column\n",
        "  conll_df['ment_id'] = conll_df.apply(lambda x: get_mention_idx(x.cluster_id, x.mention), axis=1)\n",
        "\n",
        "  #Extracting a list of mentions and clusters\n",
        "  mention_ids = []\n",
        "  mention_locs = []\n",
        "  mention_tuples = []\n",
        "  mention_nums = []\n",
        "  mention_num = -1\n",
        "  end_patts = []\n",
        "\n",
        "  for i in range(len(conll_df['tokens'])):\n",
        "    if conll_df['ment_id'][i] != 'non-mention':\n",
        "\n",
        "      mention_starts = re.findall(r'\\(\\d+', conll_df['ment_id'][i])\n",
        "\n",
        "      for j, start in enumerate(mention_starts):\n",
        "\n",
        "        mention_num += 1\n",
        "        mention_nums.append(mention_num)\n",
        "        mention_id = start[1:]\n",
        "        end_patt = mention_id + ')'\n",
        "        mention_ids.append(mention_id)\n",
        "        mention_loc = []\n",
        "        mention_tuple = ()\n",
        "        mention_loc, mention_tuple = extract_mentions(i, conll_df['ment_id'], conll_df['tokens'], mention_loc, mention_tuple, end_patt)\n",
        "        mention_locs.append(mention_loc)\n",
        "        mention_tuples.append(mention_tuple)\n",
        "\n",
        "\n",
        "  #Datframe of mentions\n",
        "  ment_df = pd.DataFrame(list(zip(mention_nums, mention_ids, mention_tuples, mention_locs)),\n",
        "                columns =['mention_num', 'mention_id', 'tokens', 'rows'])\n",
        "  \n",
        "  #Matching mentions from conll file to mention types from tsv file\n",
        "  mention_types = []\n",
        "\n",
        "  for i in range(len(ment_df.index)):\n",
        "    for j in range(len(tsv_df['lower'])):\n",
        "      if (ment_df['tokens'][i] == tsv_df['tokens'][j]):\n",
        "            mention_types.append(tsv_df['ment_type'][j])\n",
        "            break\n",
        "      if j == len(tsv_df['lower']) - 1:\n",
        "        mention_types.append('unknown')\n",
        "    \n",
        "  ment_df['type'] = mention_types\n",
        "\n",
        "  #Boolean indicator for person mentions\n",
        "  conll_df['person'] = False\n",
        "\n",
        "  #Marking locations of person mentions as True\n",
        "  for i, ment_type in enumerate(ment_df['type']):\n",
        "    if ment_type == 'PER':\n",
        "      idx = ment_df['rows'][i]\n",
        "      conll_df['person'][idx] = True\n",
        "\n",
        "  \n",
        "  #Removing non-person cluster ids from nested mentions containing a person mention\n",
        "  for j in range(len(conll_df.index)):\n",
        "    if conll_df['person'][j] == True:\n",
        "      for i in range(len(ment_df.index)):\n",
        "        if ment_df['type'][i] != 'PER':\n",
        "          full_patt = r\"\\(\\b\" + str(ment_df['mention_id'][i]) + r\"\\b\\)\"\n",
        "          start_patt = r\"\\(\\b\" + str(ment_df['mention_id'][i]) + r\"\\b\"\n",
        "          end_patt = r\"\\b\" + str(ment_df['mention_id'][i]) + r\"\\b\\)\"\n",
        "\n",
        "          #Search for full pattern\n",
        "          if len(re.findall(full_patt, conll_df['ment_id'][j])) > 0:\n",
        "            conll_df['ment_id'][j] = re.sub(full_patt, '', conll_df['ment_id'][j])\n",
        "          \n",
        "          #Search for start pattern\n",
        "          elif len(re.findall(start_patt, conll_df['ment_id'][j])) > 0:\n",
        "            conll_df['ment_id'][j] = re.sub(start_patt, '', conll_df['ment_id'][j])\n",
        "\n",
        "          #Search for end pattern\n",
        "          elif len(re.findall(end_patt, conll_df['ment_id'][j])) > 0:\n",
        "            conll_df['ment_id'][j] = re.sub(end_patt, '', conll_df['ment_id'][j])\n",
        "\n",
        "          else:\n",
        "            continue\n",
        "        else:\n",
        "          continue\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  #Setting all non-person cluster ids to NaN\n",
        "  conll_df['new_cluster_id'] = conll_df.apply(lambda x: x['ment_id'] if x['person'] == True else np.nan, axis=1)\n",
        "  conll_df['new_cluster_id'] = conll_df.apply(lambda x: x['new_cluster_id'] if x['new_cluster_id'] != 'non-mention' else np.nan, axis=1)\n",
        "\n",
        "  #Ensuring int ids\n",
        "  conll_df['blank_id'] = conll_df['blank_id'].astype(int)\n",
        "  conll_df['sent_id'] = conll_df['sent_id'].astype(int)\n",
        "\n",
        "  #New conll dataframe with only person mentions tagged\n",
        "  new_conll = conll_df[['file', 'blank_id', 'sent_id', 'word', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8','new_cluster_id']]\n",
        "\n",
        "  #Exporting conll file\n",
        "  new_file_str = f'gdrive/My Drive/Colab_Notebooks/coref_extraction/new_conll/{file}.conll'\n",
        "  new_conll.to_csv(new_file_str, sep ='\\t', header=False)"
      ],
      "metadata": {
        "id": "xWH3k6HjOgBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}